#!/usr/bin/env python3
"""
Daily News Fetcher for Typing Game
Fetches RSS feeds, sanitizes HTML, normalizes punctuation to full-width.
Supports both title-only mode and full article mode.
"""

import feedparser
from bs4 import BeautifulSoup
import json
import re
import requests
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse
import time

# RSS Feed URLs
RSS_FEEDS = {
    'zh': 'https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
    'en': 'https://news.google.com/rss?hl=en&gl=US&ceid=US:en'
}

# Number of news items to fetch per language
NEWS_COUNT = 20

# Number of full articles to fetch (fewer due to longer fetch time)
ARTICLE_COUNT = 5

# Request timeout
REQUEST_TIMEOUT = 10

# User agent for requests
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'


def normalize_punctuation(text: str) -> str:
    """Convert half-width punctuation to full-width."""
    replacements = {
        ',': 'ï¼Œ',
        '.': 'ã€‚',
        '?': 'ï¼Ÿ',
        '!': 'ï¼',
        ':': 'ï¼š',
        ';': 'ï¼›',
        '(': 'ï¼ˆ',
        ')': 'ï¼‰',
    }
    
    for half, full in replacements.items():
        text = text.replace(half, full)
    
    return text


def strip_html(html_text: str) -> str:
    """Remove all HTML tags and get clean text."""
    soup = BeautifulSoup(html_text, 'html.parser')
    return soup.get_text()


def clean_whitespace(text: str) -> str:
    """Remove excessive whitespace and newlines."""
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


def follow_google_news_redirect(google_url: str) -> Optional[str]:
    """Follow Google News redirect to get the actual article URL."""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.head(google_url, headers=headers, allow_redirects=True, timeout=REQUEST_TIMEOUT)
        return response.url
    except Exception as e:
        print(f"  âš  Redirect failed: {e}")
        return None


def extract_article_content(url: str, lang: str) -> Optional[str]:
    """Extract main article content from a news URL."""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove unwanted elements
        for tag in soup.find_all(['script', 'style', 'nav', 'header', 'footer',
                                   'aside', 'iframe', 'noscript', 'form']):
            tag.decompose()

        # Try to find article content using common selectors
        content = None

        # Common article content selectors
        selectors = [
            'article',
            '[class*="article-content"]',
            '[class*="article-body"]',
            '[class*="story-body"]',
            '[class*="post-content"]',
            '[class*="entry-content"]',
            '.content',
            'main',
        ]

        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # Get all paragraph text
                paragraphs = element.find_all('p')
                if paragraphs:
                    content = ' '.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
                    if len(content) > 200:  # Minimum content length
                        break

        if not content:
            # Fallback: get all paragraphs from body
            paragraphs = soup.find_all('p')
            content = ' '.join(p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50)

        if content:
            content = clean_whitespace(content)
            if lang == 'zh':
                content = normalize_punctuation(content)

            # Limit content length for typing practice (around 500-1000 chars)
            if len(content) > 1000:
                # Try to cut at sentence boundary
                cut_point = content.rfind('ã€‚', 0, 1000)
                if cut_point == -1:
                    cut_point = content.rfind('. ', 0, 1000)
                if cut_point == -1:
                    cut_point = 1000
                content = content[:cut_point + 1]

            return content if len(content) >= 100 else None

        return None

    except Exception as e:
        print(f"  âš  Content extraction failed: {e}")
        return None


def fetch_full_articles(lang: str, count: int = ARTICLE_COUNT) -> List[Dict]:
    """Fetch full article content from RSS feed."""
    url = RSS_FEEDS.get(lang)
    if not url:
        raise ValueError(f"Unknown language: {lang}")

    try:
        feed = feedparser.parse(url)
        articles = []

        for entry in feed.entries:
            if len(articles) >= count:
                break

            title = entry.get('title', '')
            link = entry.get('link', '')

            if not title or not link:
                continue

            # Clean title
            title = strip_html(title)
            title = title.split(' - ')[0].strip()
            if lang == 'zh':
                title = normalize_punctuation(title)
            title = clean_whitespace(title)

            if len(title) < 10:
                continue

            print(f"  Fetching: {title[:40]}...")

            # Follow redirect to get actual URL
            actual_url = follow_google_news_redirect(link)
            if not actual_url:
                continue

            # Extract article content
            content = extract_article_content(actual_url, lang)
            if content:
                articles.append({
                    'title': title,
                    'content': content,
                    'url': actual_url
                })
                print(f"    âœ“ Got {len(content)} chars")

            # Be nice to servers
            time.sleep(0.5)

        return articles

    except Exception as e:
        print(f"Error fetching {lang} articles: {e}")
        return []


def fetch_news_titles(lang: str, count: int = NEWS_COUNT) -> List[str]:
    """Fetch and clean news titles from RSS feed."""
    url = RSS_FEEDS.get(lang)
    if not url:
        raise ValueError(f"Unknown language: {lang}")
    
    try:
        feed = feedparser.parse(url)
        
        titles = []
        for entry in feed.entries[:count * 2]:
            if len(titles) >= count:
                break
            
            title = entry.get('title', '')
            if not title:
                continue
            
            title = strip_html(title)
            title = title.split(' - ')[0].strip()
            
            if lang == 'zh':
                title = normalize_punctuation(title)
            
            title = clean_whitespace(title)
            
            if len(title) >= 15:
                titles.append(title)
        
        return titles
    
    except Exception as e:
        print(f"Error fetching {lang} news: {e}")
        return []


def fetch_daily_news(include_articles: bool = True) -> Dict:
    """Fetch news for both languages and return as JSON-ready dict."""
    today = datetime.now().strftime('%Y-%m-%d')

    # Fetch titles (for sentence mode)
    print("\nğŸ“° Fetching news titles...")
    zh_news = fetch_news_titles('zh')
    en_news = fetch_news_titles('en')

    if not zh_news:
        zh_news = [
            'ç§‘æŠ€ç™¼å±•æ—¥æ–°æœˆç•°ï¼Œäººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ã€‚',
            'å…¨çƒæš–åŒ–å•é¡Œæ—¥ç›Šåš´é‡ï¼Œå„åœ‹ç´›ç´›æå‡ºæ¸›ç¢³ç›®æ¨™ã€‚',
            'æ•™è‚²æ˜¯åœ‹å®¶ç™¼å±•çš„æ ¹æœ¬ï¼ŒåŸ¹é¤Šäººæ‰æ˜¯æœ€é‡è¦çš„æŠ•è³‡ã€‚',
            'å¥åº·é£²é£Ÿå’Œè¦å¾‹é‹å‹•æ˜¯ç¶­æŒèº«é«”å¥åº·çš„ä¸äºŒæ³•é–€ã€‚',
            'é–±è®€èƒ½å¤ é–‹æ‹“è¦–é‡ï¼Œå¢é€²çŸ¥è­˜ï¼ŒåŸ¹é¤Šç¨ç«‹æ€è€ƒèƒ½åŠ›ã€‚'
        ]

    if not en_news:
        en_news = [
            'Technology advances rapidly, transforming how we live and work.',
            'Climate change poses significant challenges to global communities.',
            'Education empowers individuals and drives economic growth.',
            'Regular exercise and balanced nutrition promote well-being.',
            'Reading expands horizons and cultivates critical thinking.'
        ]

    result = {
        'date': today,
        'zh': zh_news,
        'en': en_news
    }

    # Fetch full articles (for article mode)
    if include_articles:
        print("\nğŸ“„ Fetching full articles (Chinese)...")
        zh_articles = fetch_full_articles('zh')

        print("\nğŸ“„ Fetching full articles (English)...")
        en_articles = fetch_full_articles('en')

        # Fallback articles
        if not zh_articles:
            zh_articles = [{
                'title': 'äººå·¥æ™ºæ…§çš„ç™¼å±•èˆ‡æœªä¾†',
                'content': 'äººå·¥æ™ºæ…§æŠ€è¡“è¿‘å¹´ä¾†å–å¾—äº†çªç ´æ€§çš„é€²å±•ã€‚å¾èªéŸ³è¾¨è­˜åˆ°è‡ªç„¶èªè¨€è™•ç†ï¼Œå¾é›»è…¦è¦–è¦ºåˆ°è‡ªå‹•é§•é§›ï¼ŒAIæ­£åœ¨æ”¹è®Šæˆ‘å€‘ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚å°ˆå®¶é æ¸¬ï¼Œæœªä¾†åå¹´å…§ï¼Œäººå·¥æ™ºæ…§å°‡æœƒæ›´æ·±å…¥åœ°èå…¥æˆ‘å€‘çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¸¶ä¾†æ›´å¤šä¾¿åˆ©çš„åŒæ™‚ï¼Œä¹Ÿå°‡å¸¶ä¾†æ–°çš„æŒ‘æˆ°å’Œæ©Ÿé‡ã€‚'
            }]

        if not en_articles:
            en_articles = [{
                'title': 'The Future of Artificial Intelligence',
                'content': 'Artificial intelligence has made remarkable progress in recent years. From speech recognition to natural language processing, from computer vision to autonomous driving, AI is transforming every aspect of our lives. Experts predict that in the next decade, artificial intelligence will become even more integrated into our daily routines, bringing both new conveniences and challenges.'
            }]

        result['articles_zh'] = zh_articles
        result['articles_en'] = en_articles

    return result


def main():
    """Main function: fetch news and save to JSON file."""
    print("ğŸš€ Fetching daily news...")

    news_data = fetch_daily_news(include_articles=True)

    output_file = 'daily_news.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Saved to {output_file}")
    print(f"  Date: {news_data['date']}")
    print(f"  Chinese titles: {len(news_data['zh'])} items")
    print(f"  English titles: {len(news_data['en'])} items")
    print(f"  Chinese articles: {len(news_data.get('articles_zh', []))} items")
    print(f"  English articles: {len(news_data.get('articles_en', []))} items")

    print("\nğŸ“ Preview (Chinese titles):")
    for i, title in enumerate(news_data['zh'][:3], 1):
        print(f"  {i}. {title}")

    if news_data.get('articles_zh'):
        print("\nğŸ“„ Preview (Chinese article):")
        article = news_data['articles_zh'][0]
        print(f"  Title: {article['title']}")
        print(f"  Content: {article['content'][:100]}...")


if __name__ == '__main__':
    main()
