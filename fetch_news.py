#!/usr/bin/env python3
"""
Daily News Fetcher for Typing Game
Fetches RSS feeds, sanitizes HTML, normalizes punctuation to full-width.
Supports both title-only mode and full article mode.
Enhanced to fetch 100 technology articles per language with 2026 focus.
"""

import feedparser
from bs4 import BeautifulSoup
import json
import re
import requests
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# RSS Feed URLs - Multiple sources for better coverage
RSS_FEEDS = {
    'zh': [
        'https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
        'https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtcDZHZ0pVVnlnQVAB?hl=zh-TW&gl=TW&ceid=TW:zh-Hant',  # Tech topic
    ],
    'en': [
        'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en',
        'https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en',  # Tech topic
    ]
}

# Number of news items to fetch per language
NEWS_COUNT = 20

# Number of full articles to fetch (increased to 100)
ARTICLE_COUNT = 100

# Maximum articles to attempt (since some will fail)
MAX_ATTEMPTS = 250

# Request timeout
REQUEST_TIMEOUT = 10

# User agent for requests
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'

# Technology keywords for filtering
TECH_KEYWORDS_ZH = [
    'ç§‘æŠ€', 'AI', 'äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'ç¥ç¶“ç¶²è·¯',
    'è»Ÿé«”', 'ç¨‹å¼', 'é–‹ç™¼', 'ç·¨ç¨‹', 'ç¨‹å¼ç¢¼', 'app', 'æ‡‰ç”¨',
    'æ™¶ç‰‡', 'è™•ç†å™¨', 'åŠå°é«”', 'CPU', 'GPU',
    'ç¶²è·¯', 'è³‡å®‰', 'é§­å®¢', 'åŠ å¯†', 'å€å¡Šéˆ',
    'é›²ç«¯', 'æ•¸æ“š', 'å¤§æ•¸æ“š', 'æ¼”ç®—æ³•',
    'æ‰‹æ©Ÿ', 'é›»è…¦', 'ç­†é›»', 'å¹³æ¿', 'è£ç½®',
    'iPhone', 'Android', 'Windows', 'Mac', 'Linux',
    'Google', 'Apple', 'Microsoft', 'Meta', 'Amazon',
    'ç‰©è¯ç¶²', '5G', '6G', 'VR', 'AR', 'å…ƒå®‡å®™',
    'é‡å­', 'æ©Ÿå™¨äºº', 'è‡ªå‹•é§•é§›', 'é›»å‹•è»Š',
]

TECH_KEYWORDS_EN = [
    'tech', 'technology', 'AI', 'artificial intelligence', 'machine learning',
    'deep learning', 'neural network', 'software', 'programming', 'developer',
    'code', 'app', 'application', 'chip', 'processor', 'semiconductor',
    'CPU', 'GPU', 'network', 'cybersecurity', 'hacker', 'encryption',
    'blockchain', 'cloud', 'data', 'big data', 'algorithm',
    'smartphone', 'computer', 'laptop', 'tablet', 'device',
    'iPhone', 'Android', 'Windows', 'Mac', 'Linux',
    'Google', 'Apple', 'Microsoft', 'Meta', 'Amazon',
    'IoT', '5G', '6G', 'VR', 'AR', 'metaverse',
    'quantum', 'robot', 'autonomous', 'self-driving', 'electric vehicle',
]


def normalize_punctuation(text: str) -> str:
    """Convert half-width punctuation to full-width."""
    replacements = {
        ',': 'ï¼Œ',
        '.': 'ã€‚',
        '?': 'ï¼Ÿ',
        '!': 'ï¼',
        ':': 'ï¼š',
        ';': 'ï¼›',
        '(': 'ï¼ˆ',
        ')': 'ï¼‰',
    }
    
    for half, full in replacements.items():
        text = text.replace(half, full)
    
    return text


def strip_html(html_text: str) -> str:
    """Remove all HTML tags and get clean text."""
    soup = BeautifulSoup(html_text, 'html.parser')
    return soup.get_text()


def clean_whitespace(text: str) -> str:
    """Remove excessive whitespace and newlines."""
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


def is_tech_related(title: str, content: str, lang: str) -> bool:
    """Check if article is technology-related based on keywords."""
    text = (title + ' ' + content).lower()
    keywords = TECH_KEYWORDS_ZH if lang == 'zh' else TECH_KEYWORDS_EN

    # Count keyword matches
    matches = sum(1 for keyword in keywords if keyword.lower() in text)

    # Consider it tech-related if at least 2 keywords match
    return matches >= 2


def follow_google_news_redirect(google_url: str) -> Optional[str]:
    """Follow Google News redirect to get the actual article URL."""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.head(google_url, headers=headers, allow_redirects=True, timeout=REQUEST_TIMEOUT)
        return response.url
    except Exception as e:
        print(f"  âš  Redirect failed: {e}")
        return None


def extract_article_content(url: str, lang: str) -> Optional[str]:
    """Extract main article content from a news URL."""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove unwanted elements
        for tag in soup.find_all(['script', 'style', 'nav', 'header', 'footer',
                                   'aside', 'iframe', 'noscript', 'form']):
            tag.decompose()

        # Try to find article content using common selectors
        content = None

        # Common article content selectors
        selectors = [
            'article',
            '[class*="article-content"]',
            '[class*="article-body"]',
            '[class*="story-body"]',
            '[class*="post-content"]',
            '[class*="entry-content"]',
            '.content',
            'main',
        ]

        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # Get all paragraph text
                paragraphs = element.find_all('p')
                if paragraphs:
                    content = ' '.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
                    if len(content) > 200:  # Minimum content length
                        break

        if not content:
            # Fallback: get all paragraphs from body
            paragraphs = soup.find_all('p')
            content = ' '.join(p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50)

        if content:
            content = clean_whitespace(content)
            if lang == 'zh':
                content = normalize_punctuation(content)

            # Limit content length for typing practice (around 500-1000 chars)
            if len(content) > 1000:
                # Try to cut at sentence boundary
                cut_point = content.rfind('ã€‚', 0, 1000)
                if cut_point == -1:
                    cut_point = content.rfind('. ', 0, 1000)
                if cut_point == -1:
                    cut_point = 1000
                content = content[:cut_point + 1]

            return content if len(content) >= 100 else None

        return None

    except Exception as e:
        print(f"  âš  Content extraction failed: {e}")
        return None


def fetch_single_article(entry: Dict, lang: str) -> Optional[Dict]:
    """Fetch a single article (for parallel processing)."""
    try:
        title = entry.get('title', '')
        link = entry.get('link', '')

        if not title or not link:
            return None

        # Clean title
        title = strip_html(title)
        title = title.split(' - ')[0].strip()
        if lang == 'zh':
            title = normalize_punctuation(title)
        title = clean_whitespace(title)

        if len(title) < 10:
            return None

        # Follow redirect to get actual URL
        actual_url = follow_google_news_redirect(link)
        if not actual_url:
            return None

        # Extract article content
        content = extract_article_content(actual_url, lang)
        if not content:
            return None

        # Check if tech-related
        if not is_tech_related(title, content, lang):
            return None

        return {
            'title': title,
            'content': content,
            'url': actual_url
        }

    except Exception as e:
        return None


def fetch_full_articles(lang: str, count: int = ARTICLE_COUNT) -> List[Dict]:
    """Fetch full article content from multiple RSS feeds with parallel processing."""
    feed_urls = RSS_FEEDS.get(lang, [])
    if not feed_urls:
        raise ValueError(f"Unknown language: {lang}")

    # Convert single URL to list for backward compatibility
    if isinstance(feed_urls, str):
        feed_urls = [feed_urls]

    print(f"  Fetching from {len(feed_urls)} RSS feeds...")

    # Collect all entries from all feeds
    all_entries = []
    for feed_url in feed_urls:
        try:
            feed = feedparser.parse(feed_url)
            all_entries.extend(feed.entries)
            print(f"    Found {len(feed.entries)} entries")
        except Exception as e:
            print(f"    âš  Feed error: {e}")
            continue

    print(f"  Total entries collected: {len(all_entries)}")
    print(f"  Processing up to {MAX_ATTEMPTS} articles to get {count} tech articles...")

    # Limit to MAX_ATTEMPTS
    entries_to_process = all_entries[:MAX_ATTEMPTS]

    # Use ThreadPoolExecutor for parallel fetching
    articles = []
    completed = 0

    with ThreadPoolExecutor(max_workers=10) as executor:
        # Submit all tasks
        future_to_entry = {
            executor.submit(fetch_single_article, entry, lang): entry
            for entry in entries_to_process
        }

        # Process completed tasks
        for future in as_completed(future_to_entry):
            completed += 1
            if len(articles) >= count:
                # Cancel remaining tasks
                for f in future_to_entry:
                    f.cancel()
                break

            try:
                result = future.result()
                if result:
                    articles.append(result)
                    print(f"    âœ“ [{len(articles)}/{count}] {result['title'][:50]}... ({len(result['content'])} chars)")
            except Exception as e:
                pass

            # Progress update every 20 articles
            if completed % 20 == 0:
                print(f"    Progress: {completed}/{len(entries_to_process)} processed, {len(articles)} tech articles found")

    print(f"  Final: {len(articles)} tech articles collected")
    return articles[:count]


def fetch_news_titles(lang: str, count: int = NEWS_COUNT) -> List[str]:
    """Fetch and clean news titles from RSS feeds."""
    feed_urls = RSS_FEEDS.get(lang, [])
    if not feed_urls:
        raise ValueError(f"Unknown language: {lang}")

    # Convert single URL to list for backward compatibility
    if isinstance(feed_urls, str):
        feed_urls = [feed_urls]

    titles = []

    try:
        for feed_url in feed_urls:
            if len(titles) >= count:
                break

            feed = feedparser.parse(feed_url)

            for entry in feed.entries:
                if len(titles) >= count:
                    break

                title = entry.get('title', '')
                if not title:
                    continue

                title = strip_html(title)
                title = title.split(' - ')[0].strip()

                if lang == 'zh':
                    title = normalize_punctuation(title)

                title = clean_whitespace(title)

                # Basic tech filtering for titles
                keywords = TECH_KEYWORDS_ZH if lang == 'zh' else TECH_KEYWORDS_EN
                has_tech_keyword = any(keyword.lower() in title.lower() for keyword in keywords)

                if len(title) >= 15 and has_tech_keyword:
                    titles.append(title)

        return titles

    except Exception as e:
        print(f"Error fetching {lang} news: {e}")
        return []


def fetch_daily_news(include_articles: bool = True) -> Dict:
    """Fetch news for both languages and return as JSON-ready dict."""
    today = datetime.now().strftime('%Y-%m-%d')

    # Fetch titles (for sentence mode)
    print("\nğŸ“° Fetching news titles...")
    zh_news = fetch_news_titles('zh')
    en_news = fetch_news_titles('en')

    if not zh_news:
        zh_news = [
            'äººå·¥æ™ºæ…§æŠ€è¡“åœ¨2026å¹´æŒçºŒçªç ´ï¼Œæ·±åº¦å­¸ç¿’æ¨¡å‹è®Šå¾—æ›´åŠ å¼·å¤§ã€‚',
            'é‡å­é›»è…¦ç ”ç©¶å–å¾—é‡å¤§é€²å±•ï¼Œé‹ç®—èƒ½åŠ›é”åˆ°æ–°é«˜å³°ã€‚',
            'æ–°ä¸€ä»£æ™¶ç‰‡æŠ€è¡“å•ä¸–ï¼Œè™•ç†å™¨æ•ˆèƒ½æå‡ä¸‰å€ä»¥ä¸Šã€‚',
            'é›²ç«¯é‹ç®—å¸‚å ´æŒçºŒæ“´å¤§ï¼Œä¼æ¥­æ•¸ä½è½‰å‹åŠ é€Ÿé€²è¡Œã€‚',
            'è‡ªå‹•é§•é§›æŠ€è¡“æ—¥ç›Šæˆç†Ÿï¼Œæ™ºæ…§äº¤é€šç³»çµ±é€æ­¥æ™®åŠã€‚',
            'è™›æ“¬å¯¦å¢ƒå’Œæ“´å¢å¯¦å¢ƒæŠ€è¡“èåˆï¼Œé–‹å‰µå…¨æ–°æ‡‰ç”¨å ´æ™¯ã€‚',
            'ç¶²è·¯å®‰å…¨å¨è„…å‡ç´šï¼Œä¼æ¥­åŠ å¼·è³‡å®‰é˜²è­·æªæ–½ã€‚',
            'å€å¡ŠéˆæŠ€è¡“æ‡‰ç”¨æ“´å±•ï¼Œæ•¸ä½è³‡ç”¢ç®¡ç†æ›´åŠ ä¾¿åˆ©ã€‚',
            '5Gç¶²è·¯å…¨é¢è¦†è“‹ï¼Œ6GæŠ€è¡“ç ”ç™¼å–å¾—éšæ®µæ€§æˆæœã€‚',
            'æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å„ªåŒ–ï¼Œäººå·¥æ™ºæ…§æ›´åŠ æ™ºèƒ½åŒ–ã€‚'
        ]

    if not en_news:
        en_news = [
            'Artificial intelligence breakthroughs continue in 2026 with more powerful deep learning models.',
            'Quantum computing research achieves major milestones in computational power.',
            'Next-generation chip technology emerges with triple the processing performance.',
            'Cloud computing market expands as enterprises accelerate digital transformation.',
            'Autonomous driving technology matures with smart transportation systems.',
            'Virtual and augmented reality technologies merge creating new applications.',
            'Cybersecurity threats escalate prompting stronger enterprise protection measures.',
            'Blockchain technology applications expand with improved digital asset management.',
            '5G networks achieve full coverage while 6G research reaches key milestones.',
            'Machine learning algorithms optimize making artificial intelligence smarter.'
        ]

    result = {
        'date': today,
        'zh': zh_news,
        'en': en_news
    }

    # Fetch full articles (for article mode)
    if include_articles:
        print("\nğŸ“„ Fetching full articles (Chinese)...")
        zh_articles = fetch_full_articles('zh')

        print("\nğŸ“„ Fetching full articles (English)...")
        en_articles = fetch_full_articles('en')

        # Fallback articles (tech-focused for 2026)
        if not zh_articles or len(zh_articles) < 5:
            fallback_zh = [
                {
                    'title': '2026å¹´äººå·¥æ™ºæ…§æŠ€è¡“çš„çªç ´æ€§é€²å±•',
                    'content': 'äººå·¥æ™ºæ…§æŠ€è¡“åœ¨2026å¹´æŒçºŒçªç ´å‰µæ–°ã€‚æ·±åº¦å­¸ç¿’æ¨¡å‹çš„åƒæ•¸è¦æ¨¡é”åˆ°å‰æ‰€æœªæœ‰çš„æ°´å¹³ï¼Œç¥ç¶“ç¶²è·¯æ¶æ§‹æ›´åŠ è¤‡é›œç²¾å·§ã€‚å¾èªéŸ³è¾¨è­˜åˆ°è‡ªç„¶èªè¨€è™•ç†ï¼Œå¾é›»è…¦è¦–è¦ºåˆ°è‡ªå‹•é§•é§›ï¼ŒAIæ­£åœ¨æ”¹è®Šæˆ‘å€‘ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚å¤§å‹èªè¨€æ¨¡å‹å±•ç¾å‡ºé©šäººçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤ è™•ç†æ›´è¤‡é›œçš„ä»»å‹™ã€‚å°ˆå®¶é æ¸¬ï¼Œäººå·¥æ™ºæ…§å°‡æœƒæ›´æ·±å…¥åœ°èå…¥æˆ‘å€‘çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¸¶ä¾†æ›´å¤šä¾¿åˆ©çš„åŒæ™‚ï¼Œä¹Ÿå°‡å¸¶ä¾†æ–°çš„å€«ç†å’Œç¤¾æœƒæŒ‘æˆ°ã€‚å„åœ‹æ”¿åºœå’Œç§‘æŠ€å…¬å¸éƒ½åœ¨åŠ å¤§AIç ”ç™¼æŠ•å…¥ï¼Œç«¶çˆ­æ—¥ç›Šæ¿€çƒˆã€‚'
                },
                {
                    'title': 'é‡å­é›»è…¦çªç ´å‚³çµ±é‹ç®—æ¥µé™',
                    'content': 'é‡å­é›»è…¦æŠ€è¡“åœ¨2026å¹´å–å¾—é‡å¤§çªç ´ï¼Œé‹ç®—èƒ½åŠ›é”åˆ°æ–°çš„é‡Œç¨‹ç¢‘ã€‚ç ”ç©¶åœ˜éšŠæˆåŠŸé–‹ç™¼å‡ºæ“æœ‰ä¸€åƒå€‹é‡å­ä½å…ƒçš„ç©©å®šç³»çµ±ï¼ŒéŒ¯èª¤ç‡å¤§å¹…é™ä½ã€‚é‡å­æ¼”ç®—æ³•çš„æ‡‰ç”¨ç¯„åœä¸æ–·æ“´å±•ï¼Œå¾å¯†ç¢¼å­¸åˆ°è—¥ç‰©è¨­è¨ˆï¼Œå¾é‡‘èå»ºæ¨¡åˆ°æ°£å€™é æ¸¬ï¼Œéƒ½å±•ç¾å‡ºå·¨å¤§æ½›åŠ›ã€‚å¤šå®¶ç§‘æŠ€å·¨é ­æŠ•å…¥å¤§é‡è³‡æºé–‹ç™¼å•†ç”¨é‡å­é›»è…¦ï¼Œé è¨ˆå°‡åœ¨æœªä¾†å¹¾å¹´å…§å¯¦ç¾å•†æ¥­åŒ–æ‡‰ç”¨ã€‚é‡å­é›»è…¦çš„ç™¼å±•ä¹Ÿå¸¶ä¾†æ–°çš„å®‰å…¨æŒ‘æˆ°ï¼Œå‚³çµ±åŠ å¯†æ–¹æ³•å¯èƒ½é¢è‡¨å¨è„…ï¼Œä¿ƒä½¿ç ”ç©¶äººå“¡é–‹ç™¼é‡å­å®‰å…¨åŠ å¯†æŠ€è¡“ã€‚'
                },
                {
                    'title': 'æ–°ä¸–ä»£æ™¶ç‰‡æŠ€è¡“é©æ–°åŠå°é«”ç”¢æ¥­',
                    'content': 'åŠå°é«”ç”¢æ¥­åœ¨2026å¹´è¿ä¾†é‡å¤§æŠ€è¡“é©æ–°ã€‚æ–°ä¸€ä»£ä¸‰å¥ˆç±³è£½ç¨‹æ™¶ç‰‡æ­£å¼é‡ç”¢ï¼Œé›»æ™¶é«”å¯†åº¦å†å‰µæ–°é«˜ï¼ŒåŠŸè€—é™ä½ç™¾åˆ†ä¹‹å››åã€‚ç•°è³ªæ•´åˆæŠ€è¡“æˆç†Ÿï¼Œå°‡ä¸åŒåŠŸèƒ½çš„æ™¶ç‰‡å°è£åœ¨ä¸€èµ·ï¼Œå¤§å¹…æå‡ç³»çµ±æ•ˆèƒ½ã€‚3Då †ç–ŠæŠ€è¡“çªç ´ç“¶é ¸ï¼Œè¨˜æ†¶é«”é »å¯¬æå‡æ•¸å€ã€‚AIå°ˆç”¨æ™¶ç‰‡æ•ˆèƒ½æŒçºŒå¢é•·ï¼Œç¥ç¶“ç¶²è·¯è™•ç†å™¨æ¶æ§‹ä¸æ–·å„ªåŒ–ã€‚å„åœ‹åŠ å¼·åŠå°é«”è‡ªä¸»ç ”ç™¼èƒ½åŠ›ï¼Œä¾›æ‡‰éˆå®‰å…¨æˆç‚ºæˆ°ç•¥é‡é»ã€‚ææ–™ç§‘å­¸çš„é€²æ­¥ç‚ºä¸‹ä¸€ä»£æ™¶ç‰‡æŠ€è¡“å¥ å®šåŸºç¤ï¼ŒçŸ³å¢¨çƒ¯å’Œç¢³å¥ˆç±³ç®¡ç­‰æ–°ææ–™å±•ç¾æ‡‰ç”¨å‰æ™¯ã€‚'
                },
                {
                    'title': 'é›²ç«¯é‹ç®—èˆ‡é‚Šç·£é‹ç®—åŠ é€Ÿæ•¸ä½è½‰å‹',
                    'content': 'é›²ç«¯é‹ç®—å¸‚å ´åœ¨2026å¹´æŒçºŒå¿«é€Ÿå¢é•·ï¼Œä¼æ¥­æ•¸ä½è½‰å‹å…¨é¢åŠ é€Ÿã€‚æ··åˆé›²æ¶æ§‹æˆç‚ºä¸»æµï¼Œä¼æ¥­å¯ä»¥éˆæ´»åœ°åœ¨å…¬æœ‰é›²å’Œç§æœ‰é›²ä¹‹é–“èª¿é…è³‡æºã€‚å®¹å™¨åŒ–å’Œå¾®æœå‹™æ¶æ§‹æ™®åŠï¼Œæ‡‰ç”¨ç¨‹å¼éƒ¨ç½²æ›´åŠ éˆæ´»é«˜æ•ˆã€‚é‚Šç·£é‹ç®—æŠ€è¡“æ—¥ç›Šæˆç†Ÿï¼Œåœ¨ç‰©è¯ç¶²è¨­å‚™ä¸Šé€²è¡Œå³æ™‚æ•¸æ“šè™•ç†ï¼Œé™ä½å»¶é²å’Œé »å¯¬éœ€æ±‚ã€‚ç„¡ä¼ºæœå™¨é‹ç®—æ¨¡å¼å—åˆ°é’çï¼Œé–‹ç™¼è€…åªéœ€å°ˆæ³¨æ–¼ç¨‹å¼ç¢¼é‚è¼¯ï¼Œç„¡éœ€ç®¡ç†åº•å±¤åŸºç¤è¨­æ–½ã€‚é›²ç«¯å®‰å…¨æŠ€è¡“ä¸æ–·å¼·åŒ–ï¼Œé›¶ä¿¡ä»»æ¶æ§‹é€æ­¥æ¨å»£ã€‚å¤šé›²ç®¡ç†å·¥å…·å¹«åŠ©ä¼æ¥­å„ªåŒ–è³‡æºä½¿ç”¨å’Œæˆæœ¬æ§åˆ¶ã€‚'
                },
                {
                    'title': 'ç¶²è·¯å®‰å…¨å¨è„…å‡ç´šæ¨å‹•é˜²è­·æŠ€è¡“å‰µæ–°',
                    'content': '2026å¹´ç¶²è·¯å®‰å…¨å½¢å‹¢æ—¥ç›Šåš´å³»ï¼Œæ”»æ“Šæ‰‹æ®µæ›´åŠ è¤‡é›œå¤šæ¨£ã€‚äººå·¥æ™ºæ…§è¢«æ‡‰ç”¨æ–¼æ”»æ“Šå’Œé˜²ç¦¦å…©ç«¯ï¼ŒAIé©…å‹•çš„æƒ¡æ„è»Ÿé«”èƒ½å¤ è‡ªå‹•èª¿æ•´ç­–ç•¥ç¹éé˜²è­·ï¼Œè€ŒAIå®‰å…¨ç³»çµ±ä¹Ÿåœ¨å³æ™‚åµæ¸¬å’Œæ‡‰å°å¨è„…ã€‚å‹’ç´¢è»Ÿé«”æ”»æ“ŠæŒçºŒå¢åŠ ï¼Œç›®æ¨™å¾å€‹äººæ“´å±•åˆ°ä¼æ¥­å’Œé—œéµåŸºç¤è¨­æ–½ã€‚é›¶ä¿¡ä»»å®‰å…¨æ¶æ§‹æˆç‚ºä¼æ¥­æ¨™æº–é…ç½®ï¼Œä¸å†å‡è¨­å…§ç¶²ç’°å¢ƒå®‰å…¨ã€‚é‡å­åŠ å¯†æŠ€è¡“é–‹å§‹éƒ¨ç½²ï¼Œç‚ºæœªä¾†é‡å­é›»è…¦å¨è„…åšå¥½æº–å‚™ã€‚ç”Ÿç‰©è­˜åˆ¥å’Œå¤šå› ç´ èªè­‰æ™®åŠï¼Œå¯†ç¢¼ç®¡ç†æ–¹å¼æ”¹è®Šã€‚å®‰å…¨æ„è­˜åŸ¹è¨“å—åˆ°é‡è¦–ï¼Œäººç‚ºå› ç´ ä»æ˜¯æœ€å¤§å®‰å…¨æ¼æ´ã€‚'
                }
            ]
            zh_articles = fallback_zh if not zh_articles else zh_articles + fallback_zh[:5 - len(zh_articles)]

        if not en_articles or len(en_articles) < 5:
            fallback_en = [
                {
                    'title': 'Breakthrough Advances in Artificial Intelligence in 2026',
                    'content': 'Artificial intelligence technology continues to break new ground in 2026. Deep learning models have reached unprecedented parameter scales with increasingly sophisticated neural network architectures. From speech recognition to natural language processing, from computer vision to autonomous driving, AI is transforming every aspect of our lives. Large language models demonstrate remarkable understanding and generation capabilities, handling more complex tasks than ever before. Experts predict that artificial intelligence will become even more deeply integrated into our daily lives, bringing new conveniences alongside ethical and social challenges. Governments and tech companies worldwide are increasing AI research investments, with competition intensifying rapidly.'
                },
                {
                    'title': 'Quantum Computing Breaks Traditional Computational Limits',
                    'content': 'Quantum computing technology achieved major breakthroughs in 2026, reaching new milestones in computational power. Research teams successfully developed stable systems with one thousand qubits, significantly reducing error rates. The application scope of quantum algorithms continues to expand, from cryptography to drug design, from financial modeling to climate prediction, demonstrating enormous potential. Multiple tech giants are investing heavily in developing commercial quantum computers, expected to achieve commercialization within the next few years. The development of quantum computing also brings new security challenges, as traditional encryption methods may face threats, prompting researchers to develop quantum-safe cryptography.'
                },
                {
                    'title': 'Next-Generation Chip Technology Revolutionizes Semiconductor Industry',
                    'content': 'The semiconductor industry welcomed major technological innovations in 2026. New generation three-nanometer process chips entered mass production, with transistor density reaching new highs and power consumption reduced by forty percent. Heterogeneous integration technology matured, packaging chips with different functions together and dramatically improving system performance. 3D stacking technology broke through bottlenecks, multiplying memory bandwidth. AI-specific chip performance continues to grow with constantly optimized neural network processor architectures. Countries are strengthening semiconductor self-sufficiency capabilities, making supply chain security a strategic priority. Advances in materials science lay the foundation for next-generation chip technology, with graphene and carbon nanotubes showing application promise.'
                },
                {
                    'title': 'Cloud and Edge Computing Accelerate Digital Transformation',
                    'content': 'The cloud computing market continued rapid growth in 2026 as enterprise digital transformation fully accelerated. Hybrid cloud architectures became mainstream, allowing enterprises to flexibly allocate resources between public and private clouds. Containerization and microservices architectures proliferated, making application deployment more flexible and efficient. Edge computing technology matured, enabling real-time data processing on IoT devices while reducing latency and bandwidth requirements. Serverless computing models gained favor, allowing developers to focus solely on code logic without managing underlying infrastructure. Cloud security technologies continuously strengthened with zero-trust architectures gradually spreading. Multi-cloud management tools help enterprises optimize resource usage and cost control.'
                },
                {
                    'title': 'Escalating Cybersecurity Threats Drive Defense Innovation',
                    'content': 'The cybersecurity situation in 2026 became increasingly severe with attack methods growing more complex and diverse. Artificial intelligence is being applied to both attack and defense, with AI-driven malware automatically adjusting strategies to bypass protections, while AI security systems detect and respond to threats in real-time. Ransomware attacks continued to increase, with targets expanding from individuals to enterprises and critical infrastructure. Zero-trust security architectures became standard enterprise configurations, no longer assuming internal network safety. Quantum encryption technology began deployment, preparing for future quantum computer threats. Biometric and multi-factor authentication proliferated, changing password management practices. Security awareness training gained attention, as human factors remain the biggest security vulnerability.'
                }
            ]
            en_articles = fallback_en if not en_articles else en_articles + fallback_en[:5 - len(en_articles)]

        result['articles_zh'] = zh_articles
        result['articles_en'] = en_articles

    return result


def main():
    """Main function: fetch news and save to JSON file."""
    print("ğŸš€ Fetching 2026 technology news...")
    print(f"   Target: {ARTICLE_COUNT} articles per language")
    print(f"   This may take 10-30 minutes depending on network speed...\n")

    start_time = time.time()
    news_data = fetch_daily_news(include_articles=True)
    elapsed = time.time() - start_time

    output_file = 'daily_news.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Saved to {output_file}")
    print(f"  Date: {news_data['date']}")
    print(f"  Time elapsed: {elapsed:.1f} seconds")
    print(f"  Chinese titles: {len(news_data['zh'])} items")
    print(f"  English titles: {len(news_data['en'])} items")
    print(f"  Chinese articles: {len(news_data.get('articles_zh', []))} items")
    print(f"  English articles: {len(news_data.get('articles_en', []))} items")

    # Calculate statistics
    if news_data.get('articles_zh'):
        zh_avg_len = sum(len(a['content']) for a in news_data['articles_zh']) / len(news_data['articles_zh'])
        print(f"  Average Chinese article length: {zh_avg_len:.0f} characters")

    if news_data.get('articles_en'):
        en_avg_len = sum(len(a['content']) for a in news_data['articles_en']) / len(news_data['articles_en'])
        print(f"  Average English article length: {en_avg_len:.0f} characters")

    print("\nğŸ“ Preview (Chinese titles):")
    for i, title in enumerate(news_data['zh'][:3], 1):
        print(f"  {i}. {title}")

    if news_data.get('articles_zh'):
        print("\nğŸ“„ Preview (Chinese article):")
        article = news_data['articles_zh'][0]
        print(f"  Title: {article['title']}")
        print(f"  Content: {article['content'][:100]}...")

    print("\nğŸ“ Preview (English titles):")
    for i, title in enumerate(news_data['en'][:3], 1):
        print(f"  {i}. {title}")

    if news_data.get('articles_en'):
        print("\nğŸ“„ Preview (English article):")
        article = news_data['articles_en'][0]
        print(f"  Title: {article['title']}")
        print(f"  Content: {article['content'][:100]}...")


if __name__ == '__main__':
    main()
